# DART

This show the core of our DART model.

### Training stage
1. We provide pretrained HQ and LQ encoder in link. The HQ encoder is freezed while the LQ encoder is trainable during training.
2. We use an alignment module which is in line with the I2T prior in our paper.This module needs text description of FFHQ dataset. You can generate the text from any VLMs. Remember to change the path of your texts in 'alignment_hq.py'
3. run 'bash scripts/run_DART_training.sh'

### Inference stage
1. We provide pretrained DART in link.
2. We use an alignment module which is in line with the I2T prior in our paper.This module needs text description of images you test. Here we provide a few of them in './models/DART/test_lfw' generated by LLaVA. Alse remember to change the path of your texts in 'alignment_hq.py'
3. Use pictures in './models/DART/DART/data/test_lfw' and run 'bash scripts/test.sh'
